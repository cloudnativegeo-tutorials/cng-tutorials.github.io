[
  {
    "objectID": "tutorials/landsat-from-mspc/index.html",
    "href": "tutorials/landsat-from-mspc/index.html",
    "title": "CNG Tutorials",
    "section": "",
    "text": "How to run this tutorial\n\n\n\nIn order to run the code in this tutorial, you can either download the notebook to run it on your local computer, or click the button below to run the tutorial in a GitHub Codespace."
  },
  {
    "objectID": "tutorials/landsat-from-mspc/index.html#reading-data-from-the-stac-api",
    "href": "tutorials/landsat-from-mspc/index.html#reading-data-from-the-stac-api",
    "title": "CNG Tutorials",
    "section": "Reading Data from the STAC API",
    "text": "Reading Data from the STAC API\nThe Planetary Computer catalogs the datasets we host using the STAC (SpatioTemporal Asset Catalog) specification. We provide a STAC API endpoint that can be used to search our datasets by space, time, and more. This quickstart will show you how to search for data using our STAC API and open-source Python libraries. For more on how to use our STAC API from R, see Reading data from the STAC API with R.\nFirst we’ll use pystac-client to open up our STAC API:\n\nfrom pystac_client import Client\n\ncatalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n\n\nSearching\nWe can use the STAC API to search for assets meeting some criteria. This might include the date and time the asset covers, is spatial extent, or any other property captured in the STAC item’s metadata.\nIn this example we’ll search for imagery from Landsat Collection 2 Level-2 area around Microsoft’s main campus in December of 2020.\n\ntime_range = \"2020-12-01/2020-12-31\"\nbbox = [-122.2751, 47.5469, -121.9613, 47.7458]\n\nsearch = catalog.search(collections=[\"landsat-8-c2-l2\"], bbox=bbox, datetime=time_range)\nitems = search.get_all_items()\nlen(items)\n\nIn that example our spatial query used a bounding box with a bbox. Alternatively, you can pass a GeoJSON object as intersects\narea_of_interest = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n        [\n            [-122.2751, 47.5469],\n            [-121.9613, 47.9613],\n            [-121.9613, 47.9613],\n            [-122.2751, 47.9613],\n            [-122.2751, 47.5469],\n        ]\n    ],\n}\n\ntime_range = \"2020-12-01/2020-12-31\"\n\nsearch = catalog.search(\n    collections=[\"landsat-8-c2-l2\"], intersects=area_of_interest, datetime=time_range\n)\nitems is a pystac.ItemCollection. We can see that 4 items matched our search criteria.\n\nlen(items)\n\nEach pystac.Item in this ItemCollection includes all the metadata for that scene. STAC Items are GeoJSON features, and so can be loaded by libraries like geopandas.\n\nimport geopandas\n\ndf = geopandas.GeoDataFrame.from_features(items.to_dict(), crs=\"epsg:4326\")\ndf\n\nWe can use the eo extension to sort the items by cloudiness. We’ll grab an item with low cloudiness:\n\nselected_item = min(items, key=lambda item: item.properties[\"eo:cloud_cover\"])\nselected_item\n\nEach STAC item has one or more Assets, which include links to the actual files.\n\nimport rich.table\n\ntable = rich.table.Table(\"Asset Key\", \"Descripiption\")\nfor asset_key, asset in selected_item.assets.items():\n    # print(f\"{asset_key:&lt;25} - {asset.title}\")\n    table.add_row(asset_key, asset.title)\n\ntable\n\nHere, we’ll inspect the thumbnail asset.\n\nselected_item.assets[\"thumbnail\"].to_dict()\n\n\nfrom IPython.display import Image\n\nImage(url=selected_item.assets[\"thumbnail\"].href, width=500)\n\nThat rendered_preview asset is generated dynamically from the raw data using the Planetary Computer’s data API. We can access the raw data, stored as Cloud Optimzied GeoTIFFs in Azure Blob Storage, using one of the other assets. That said, we do need to do one more thing before accessing the data. If we simply made a request to the file in blob storage we’d get a 404:\n\nimport requests\n\nurl = selected_item.assets[\"SR_B2\"].href\nprint(\"Accessing\", url)\nresponse = requests.get(url)\nresponse\n\nThat’s because the Plantary Computer uses Azure Blob Storage SAS Tokens to enable access to our data, which allows us to provide the data for free to anyone, anywhere while maintaining some control over the amount of egress for datasets.\nTo get a token for access, you can use the Planetary Computer’s Data Authentication API. You can access that anonymously, or you can provide an API Key for higher rate limits and longer-lived tokens.\nYou can also use the planetary-computer package to generate tokens and sign asset HREFs for access. You can install via pip with\n&gt; pip install planetary-computer\n\nimport planetary_computer\n\n# PC_SDK_SUBSCRIPTION_KEY\nsigned_href = planetary_computer.sign(selected_item).assets[\"SR_B2\"].href\n\nWe can load up that single COG using libraries like rioxarray or rasterio\n\n# import xarray as xr\nimport rioxarray\n\nds = rioxarray.open_rasterio(signed_href, overview_level=4).squeeze()\nimg = ds.plot(cmap=\"Blues\", add_colorbar=False)\nimg.axes.set_axis_off();\n\nIf you wish to work with multiple STAC items as a datacube, you can use libraries like stackstac or odc-stac.\n\nimport stackstac\n\nds = stackstac.stack(\n  planetary_computer.sign(items),\n  epsg=4326\n)\nds\n\n\n\nSearching on additional properties\nPreviously, we searched for items by space and time. Because the Planetary Computer’s STAC API supports the query parameter, you can search on additional properties on the STAC item.\nFor example, collections like sentinel-2-l2a and landsat-8-c2-l2 both implement the eo STAC extension and include an eo:cloud_cover property. Use query={\"eo:cloud_cover\": {\"lt\": 20}} to return only items that are less than 20% cloudy.\n\ntime_range = \"2020-12-01/2020-12-31\"\nbbox = [-122.2751, 47.5469, -121.9613, 47.7458]\n\nsearch = catalog.search(\n    collections=[\"sentinel-2-l2a\"],\n    bbox=bbox,\n    datetime=time_range,\n    query={\"eo:cloud_cover\": {\"lt\": 20}},\n)\nitems = search.get_all_items()\n\nOther common uses of the query parameter is to filter a collection down to items of a specific type, For example, the GOES-CMI collection includes images from various when the satellite is in various modes, which produces images of either the Full Disk of the earth, the continental United States, or a mesoscale. You can use goes:image-type to filter down to just the ones you want.\n\nsearch = catalog.search(\n    collections=[\"goes-cmi\"],\n    bbox=[-67.2729, 25.6000, -61.7999, 27.5423],\n    datetime=[\"2018-09-11T13:00:00Z\", \"2018-09-11T15:40:00Z\"],\n    query={\"goes:image-type\": {\"eq\": \"MESOSCALE\"}},\n)\n\n\n\nAnalyzing STAC Metadata\nSTAC items are proper GeoJSON Features, and so can be treated as a kind of data on their own.\n\nsearch = catalog.search(\n    collections=[\"sentinel-2-l2a\"],\n    bbox=[-124.2751, 45.5469, -110.9613, 47.7458],\n    datetime=\"2020-12-26/2020-12-31\",\n)\nitems = search.get_all_items()\n\ndf = geopandas.GeoDataFrame.from_features(items.to_dict(), crs=\"epsg:4326\")\n\ndf[[\"geometry\", \"datetime\", \"s2:mgrs_tile\", \"eo:cloud_cover\"]].explore(\n    column=\"eo:cloud_cover\", style_kwds={\"fillOpacity\": 0.1}\n)\n\nOr we can plot cloudiness of a region over time.\n\nimport pandas as pd\n\nsearch = catalog.search(\n    collections=[\"sentinel-2-l2a\"],\n    bbox=[-124.2751, 45.5469, -123.9613, 45.7458],\n    datetime=\"2020-01-01/2020-12-31\",\n)\nitems = search.get_all_items()\ndf = geopandas.GeoDataFrame.from_features(items.to_dict())\ndf[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n\n\nts = df.set_index(\"datetime\").sort_index()[\"eo:cloud_cover\"].rolling(7).mean()\nts.plot(title=\"eo:cloud-cover (7-scene rolling average)\");\n\n\n\nWorking with STAC Catalogs and Collections\nOur catalog is a STAC Catalog that we can crawl or search. The Catalog contains STAC Collections for each dataset we have indexed (which is not the yet the entirity of data hosted by the Planetary Computer).\nCollections have information about the STAC Items they contain. For instance, here we look at the Bands available for Landsat Collection 2 Level 2 data:\n\nimport pandas as pd\n\nlandsat = catalog.get_collection(\"landsat-c2-l2\")\n\nbands = [k for k,v in landsat.extra_fields['item_assets'].items() if 'data' in v['roles']]\npd.DataFrame(bands)\n\nWe can see what Assets are available on our item with:\n\npd.DataFrame.from_dict(landsat.extra_fields[\"item_assets\"], orient=\"index\")[\n    [\"title\", \"description\", \"gsd\"]\n]\n\nSome collections, like Daymet include collection-level assets. You can use the .assets property to access those assets.\n\ncollection = catalog.get_collection(\"daymet-daily-na\")\ncollection\n\nJust like assets on items, these assets include links to data in Azure Blob Storage.\n\nasset = collection.assets[\"zarr-https\"]\nasset\n\n\nimport fsspec\nimport xarray as xr\n\nstore = fsspec.get_mapper(asset.href)\nds = xr.open_zarr(store, **asset.extra_fields[\"xarray:open_kwargs\"])\nds"
  },
  {
    "objectID": "tutorials/landsat-from-mspc.html",
    "href": "tutorials/landsat-from-mspc.html",
    "title": "CNG Tutorials",
    "section": "",
    "text": "How to run this tutorial\n\n\n\nIn order to run the code in this tutorial, you can either download the notebook to run it on your local computer, or click the button below to run the tutorial in a GitHub Codespace.\n\n\n\n\n\nThe Planetary Computer catalogs the datasets we host using the STAC (SpatioTemporal Asset Catalog) specification. We provide a STAC API endpoint that can be used to search our datasets by space, time, and more. This quickstart will show you how to search for data using our STAC API and open-source Python libraries. For more on how to use our STAC API from R, see Reading data from the STAC API with R.\nFirst we’ll use pystac-client to open up our STAC API:\n\n\nWe can use the STAC API to search for assets meeting some criteria. This might include the date and time the asset covers, is spatial extent, or any other property captured in the STAC item’s metadata.\nIn this example we’ll search for imagery from Landsat Collection 2 Level-2 area around Microsoft’s main campus in December of 2020.\nIn that example our spatial query used a bounding box with a bbox. Alternatively, you can pass a GeoJSON object as intersects\narea_of_interest = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n        [\n            [-122.2751, 47.5469],\n            [-121.9613, 47.9613],\n            [-121.9613, 47.9613],\n            [-122.2751, 47.9613],\n            [-122.2751, 47.5469],\n        ]\n    ],\n}\n\ntime_range = \"2020-12-01/2020-12-31\"\n\nsearch = catalog.search(\n    collections=[\"landsat-8-c2-l2\"], intersects=area_of_interest, datetime=time_range\n)\nitems is a pystac.ItemCollection. We can see that 4 items matched our search criteria.\nEach pystac.Item in this ItemCollection includes all the metadata for that scene. STAC Items are GeoJSON features, and so can be loaded by libraries like geopandas.\nWe can use the eo extension to sort the items by cloudiness. We’ll grab an item with low cloudiness:\nEach STAC item has one or more Assets, which include links to the actual files.\nHere, we’ll inspect the thumbnail asset.\nThat rendered_preview asset is generated dynamically from the raw data using the Planetary Computer’s data API. We can access the raw data, stored as Cloud Optimzied GeoTIFFs in Azure Blob Storage, using one of the other assets. That said, we do need to do one more thing before accessing the data. If we simply made a request to the file in blob storage we’d get a 404:\nThat’s because the Plantary Computer uses Azure Blob Storage SAS Tokens to enable access to our data, which allows us to provide the data for free to anyone, anywhere while maintaining some control over the amount of egress for datasets.\nTo get a token for access, you can use the Planetary Computer’s Data Authentication API. You can access that anonymously, or you can provide an API Key for higher rate limits and longer-lived tokens.\nYou can also use the planetary-computer package to generate tokens and sign asset HREFs for access. You can install via pip with\n&gt; pip install planetary-computer\nWe can load up that single COG using libraries like rioxarray or rasterio\nIf you wish to work with multiple STAC items as a datacube, you can use libraries like stackstac or odc-stac.\n\n\n\nPreviously, we searched for items by space and time. Because the Planetary Computer’s STAC API supports the query parameter, you can search on additional properties on the STAC item.\nFor example, collections like sentinel-2-l2a and landsat-8-c2-l2 both implement the eo STAC extension and include an eo:cloud_cover property. Use query={\"eo:cloud_cover\": {\"lt\": 20}} to return only items that are less than 20% cloudy.\nOther common uses of the query parameter is to filter a collection down to items of a specific type, For example, the GOES-CMI collection includes images from various when the satellite is in various modes, which produces images of either the Full Disk of the earth, the continental United States, or a mesoscale. You can use goes:image-type to filter down to just the ones you want.\n\n\n\nSTAC items are proper GeoJSON Features, and so can be treated as a kind of data on their own.\nOr we can plot cloudiness of a region over time.\n\n\n\nOur catalog is a STAC Catalog that we can crawl or search. The Catalog contains STAC Collections for each dataset we have indexed (which is not the yet the entirity of data hosted by the Planetary Computer).\nCollections have information about the STAC Items they contain. For instance, here we look at the Bands available for Landsat Collection 2 Level 2 data:\nWe can see what Assets are available on our item with:\nSome collections, like Daymet include collection-level assets. You can use the .assets property to access those assets.\nJust like assets on items, these assets include links to data in Azure Blob Storage.\n\n\nSource: How to run this tutorial",
    "crumbs": [
      "Home",
      "Landsat from MSPC"
    ]
  },
  {
    "objectID": "tutorials/landsat-from-mspc.html#reading-data-from-the-stac-api",
    "href": "tutorials/landsat-from-mspc.html#reading-data-from-the-stac-api",
    "title": "CNG Tutorials",
    "section": "",
    "text": "The Planetary Computer catalogs the datasets we host using the STAC (SpatioTemporal Asset Catalog) specification. We provide a STAC API endpoint that can be used to search our datasets by space, time, and more. This quickstart will show you how to search for data using our STAC API and open-source Python libraries. For more on how to use our STAC API from R, see Reading data from the STAC API with R.\nFirst we’ll use pystac-client to open up our STAC API:\n\n\nWe can use the STAC API to search for assets meeting some criteria. This might include the date and time the asset covers, is spatial extent, or any other property captured in the STAC item’s metadata.\nIn this example we’ll search for imagery from Landsat Collection 2 Level-2 area around Microsoft’s main campus in December of 2020.\nIn that example our spatial query used a bounding box with a bbox. Alternatively, you can pass a GeoJSON object as intersects\narea_of_interest = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n        [\n            [-122.2751, 47.5469],\n            [-121.9613, 47.9613],\n            [-121.9613, 47.9613],\n            [-122.2751, 47.9613],\n            [-122.2751, 47.5469],\n        ]\n    ],\n}\n\ntime_range = \"2020-12-01/2020-12-31\"\n\nsearch = catalog.search(\n    collections=[\"landsat-8-c2-l2\"], intersects=area_of_interest, datetime=time_range\n)\nitems is a pystac.ItemCollection. We can see that 4 items matched our search criteria.\nEach pystac.Item in this ItemCollection includes all the metadata for that scene. STAC Items are GeoJSON features, and so can be loaded by libraries like geopandas.\nWe can use the eo extension to sort the items by cloudiness. We’ll grab an item with low cloudiness:\nEach STAC item has one or more Assets, which include links to the actual files.\nHere, we’ll inspect the thumbnail asset.\nThat rendered_preview asset is generated dynamically from the raw data using the Planetary Computer’s data API. We can access the raw data, stored as Cloud Optimzied GeoTIFFs in Azure Blob Storage, using one of the other assets. That said, we do need to do one more thing before accessing the data. If we simply made a request to the file in blob storage we’d get a 404:\nThat’s because the Plantary Computer uses Azure Blob Storage SAS Tokens to enable access to our data, which allows us to provide the data for free to anyone, anywhere while maintaining some control over the amount of egress for datasets.\nTo get a token for access, you can use the Planetary Computer’s Data Authentication API. You can access that anonymously, or you can provide an API Key for higher rate limits and longer-lived tokens.\nYou can also use the planetary-computer package to generate tokens and sign asset HREFs for access. You can install via pip with\n&gt; pip install planetary-computer\nWe can load up that single COG using libraries like rioxarray or rasterio\nIf you wish to work with multiple STAC items as a datacube, you can use libraries like stackstac or odc-stac.\n\n\n\nPreviously, we searched for items by space and time. Because the Planetary Computer’s STAC API supports the query parameter, you can search on additional properties on the STAC item.\nFor example, collections like sentinel-2-l2a and landsat-8-c2-l2 both implement the eo STAC extension and include an eo:cloud_cover property. Use query={\"eo:cloud_cover\": {\"lt\": 20}} to return only items that are less than 20% cloudy.\nOther common uses of the query parameter is to filter a collection down to items of a specific type, For example, the GOES-CMI collection includes images from various when the satellite is in various modes, which produces images of either the Full Disk of the earth, the continental United States, or a mesoscale. You can use goes:image-type to filter down to just the ones you want.\n\n\n\nSTAC items are proper GeoJSON Features, and so can be treated as a kind of data on their own.\nOr we can plot cloudiness of a region over time.\n\n\n\nOur catalog is a STAC Catalog that we can crawl or search. The Catalog contains STAC Collections for each dataset we have indexed (which is not the yet the entirity of data hosted by the Planetary Computer).\nCollections have information about the STAC Items they contain. For instance, here we look at the Bands available for Landsat Collection 2 Level 2 data:\nWe can see what Assets are available on our item with:\nSome collections, like Daymet include collection-level assets. You can use the .assets property to access those assets.\nJust like assets on items, these assets include links to data in Azure Blob Storage.",
    "crumbs": [
      "Home",
      "Landsat from MSPC"
    ]
  },
  {
    "objectID": "tutorials/s2-from-aws.html",
    "href": "tutorials/s2-from-aws.html",
    "title": "Access Sentinel 2 Data from AWS",
    "section": "",
    "text": "How to run this tutorial\n\n\n\nIn order to run the code in this tutorial, you can either download the notebook to run it on your local computer, or click the button below to run the tutorial in a GitHub Codespace.\n\n\n\n\n\nhttps://registry.opendata.aws/sentinel-2-l2a-cogs/\n\n\nThis step is optional, but it does improve load speed significantly. You don’t have to use Dask, as you can load data directly into memory of the notebook.\n\n\n\n\n\n\nWe’ll use GeoPandas DataFrame object to make plotting easier.\n\n\n\n\n\n\nNote that even though there are 9 STAC Items on input, there is only one timeslice on output. This is because of groupby=\"solar_day\". With that setting stac_load will place all items that occured on the same day (as adjusted for the timezone) into one image plane.\n\n\n\n\n\n\n\n\n\nAs you can see stac_load returned all the data covered by STAC items returned from the query. This happens by default as stac_load has no way of knowing what your query was. But it is possible to control what region is loaded. There are several mechanisms available, but probably simplest one is to use bbox= parameter (compatible with stac_client).\nLet’s load a small region at native resolution to demonstrate.\n\n\n\nSource: How to run this tutorial",
    "crumbs": [
      "Home",
      "S2 from AWS"
    ]
  },
  {
    "objectID": "tutorials/s2-from-aws.html#start-dask-client",
    "href": "tutorials/s2-from-aws.html#start-dask-client",
    "title": "Access Sentinel 2 Data from AWS",
    "section": "",
    "text": "This step is optional, but it does improve load speed significantly. You don’t have to use Dask, as you can load data directly into memory of the notebook.",
    "crumbs": [
      "Home",
      "S2 from AWS"
    ]
  },
  {
    "objectID": "tutorials/s2-from-aws.html#review-query-result",
    "href": "tutorials/s2-from-aws.html#review-query-result",
    "title": "Access Sentinel 2 Data from AWS",
    "section": "",
    "text": "We’ll use GeoPandas DataFrame object to make plotting easier.",
    "crumbs": [
      "Home",
      "S2 from AWS"
    ]
  },
  {
    "objectID": "tutorials/s2-from-aws.html#construct-dask-dataset",
    "href": "tutorials/s2-from-aws.html#construct-dask-dataset",
    "title": "Access Sentinel 2 Data from AWS",
    "section": "",
    "text": "Note that even though there are 9 STAC Items on input, there is only one timeslice on output. This is because of groupby=\"solar_day\". With that setting stac_load will place all items that occured on the same day (as adjusted for the timezone) into one image plane.",
    "crumbs": [
      "Home",
      "S2 from AWS"
    ]
  },
  {
    "objectID": "tutorials/s2-from-aws.html#load-with-bounding-box",
    "href": "tutorials/s2-from-aws.html#load-with-bounding-box",
    "title": "Access Sentinel 2 Data from AWS",
    "section": "",
    "text": "As you can see stac_load returned all the data covered by STAC items returned from the query. This happens by default as stac_load has no way of knowing what your query was. But it is possible to control what region is loaded. There are several mechanisms available, but probably simplest one is to use bbox= parameter (compatible with stac_client).\nLet’s load a small region at native resolution to demonstrate.",
    "crumbs": [
      "Home",
      "S2 from AWS"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cloud Native Geospatial (CNG) Tutorials",
    "section": "",
    "text": "This repository contains code for creating a website of tutorials about Cloud-Native Geospatial (CNG) technologies."
  },
  {
    "objectID": "index.html#viewing",
    "href": "index.html#viewing",
    "title": "Cloud Native Geospatial (CNG) Tutorials",
    "section": "Viewing",
    "text": "Viewing\nThe tutorials are hosted as static webpages on the website: vorgeo.github.io/cng-onboarding/ To browse through the available tutorials, use the navigation menu on the left side."
  },
  {
    "objectID": "index.html#executing-the-code",
    "href": "index.html#executing-the-code",
    "title": "Cloud Native Geospatial (CNG) Tutorials",
    "section": "Executing the Code",
    "text": "Executing the Code\n\nLocal Execution (with Pixi)\nRequirements:\n\nLocal install of git\nLocal install of pixi\n\nSteps:\n\nClone the repo: git clone https://github.com/VorGeo/cng-onboarding.git\nNavigate to a tutorial directory. For example: cd tutorials/stac/python/python-read-stac/\nRun the start task to start up JupyterLab: pixi run start\n\nThis will install the tutorial’s code dependencies, start a JupyterLab server, and open up the tutorial in a browser window.\n\n\n\n\nLocal Execution (with Dev Containers)\nRequirements:\n\nLocal install of git\nLocal install of pixi\nLocal install of VS Code with the following extensions: Dev Containers, Docker\n\nSteps:\n\nClone the repo: git clone https://github.com/VorGeo/cng-onboarding.git\nOpen the cloned repo in VS Code.\nOpen the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac) and select “Dev Containers: Rebuild and Reopen in Container”.\n\nSelect the Container for the tutorial you want to run.\n\nOnce the Container is built, you can open up the notebook (index.ipynb) from the Explorer menu.\n\n\n\nCloud Execution (with GitHub Codespaces)\nRequirements:\n\nA GitHub account\n\nSteps:\n\nWithin the website (https://vorgeo.github.io/cng-onboarding/), open one of the tutorial pages.\nClick on the ‘Open in GitHub Codespaces’ button.\nIn the “Create codespace” page, modify the selection options as needed and then click “Create codespace”.\nA new Codespace will be started. It can take up to 5 minutes for the codespace to be configured, at which point you will see the message “Finished configuring codespace.” in the terminal tab.\nOnce the codespace is ready, you can interact with the tutorial in the same way as you would in a local environment. Open up the notebook (index.ipynb) and start executing the cells.\n\nYou may need to specify the kernel to use for the notebook. To do this, click on the kernel name in the top right corner of the notebook and select the default kernel (e.g., .pixi/envs/default/bin/python)."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Cloud Native Geospatial (CNG) Tutorials",
    "section": "Contributing",
    "text": "Contributing\n\nContributing to an existing tutorial\n[TODO… Describe how to contribute changes to a tutorial.]\n\n\nCreating a new tutorial\n\nCreate a new tutorial repo by cloning the template repo\nDraft the tutorial\n\nUpdate the pixi environment by either running pixi add &lt;PACKAGE&gt; or by editing the pyproject.toml file.\nEdit the index.ipynb file to add tutorial content.\nPreview the tutorial by running quarto preview in the tutorial directory.\n\nAdd the tutorial to the main website\n\nAdd the tutorial repository to this repository as a submodule\n\ngit submodule add &lt;TUTORIAL_REPO_URL&gt; tutorials/&lt;TUTORIAL_NAME&gt;\ngit submodule update --init --recursive\n\nAdd the tutorial to navigation by editing the _quarto.yml file"
  },
  {
    "objectID": "tutorials/s2-from-aws/index.html",
    "href": "tutorials/s2-from-aws/index.html",
    "title": "Access Sentinel 2 Data from AWS",
    "section": "",
    "text": "How to run this tutorial\n\n\n\nIn order to run the code in this tutorial, you can either download the notebook to run it on your local computer, or click the button below to run the tutorial in a GitHub Codespace."
  },
  {
    "objectID": "tutorials/s2-from-aws/index.html#start-dask-client",
    "href": "tutorials/s2-from-aws/index.html#start-dask-client",
    "title": "Access Sentinel 2 Data from AWS",
    "section": "Start Dask Client",
    "text": "Start Dask Client\nThis step is optional, but it does improve load speed significantly. You don’t have to use Dask, as you can load data directly into memory of the notebook.\n\nclient = dask.distributed.Client()\nconfigure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client)\ndisplay(client)"
  },
  {
    "objectID": "tutorials/s2-from-aws/index.html#find-stac-items-to-load",
    "href": "tutorials/s2-from-aws/index.html#find-stac-items-to-load",
    "title": "Access Sentinel 2 Data from AWS",
    "section": "Find STAC Items to Load",
    "text": "Find STAC Items to Load\n\nkm2deg = 1.0 / 111\nx, y = (113.887, -25.843)  # Center point of a query\nr = 100 * km2deg\nbbox = (x - r, y - r, x + r, y + r)\n\ncatalog = Client.open(\"https://earth-search.aws.element84.com/v1\")\n\nquery = catalog.search(\n    collections=[\"sentinel-2-l2a\"], datetime=\"2021-09-16\", limit=100, bbox=bbox\n)\n\nitems = list(query.items())\nprint(f\"Found: {len(items):d} datasets\")\n\n# Convert STAC items into a GeoJSON FeatureCollection\nstac_json = query.item_collection_as_dict()"
  },
  {
    "objectID": "tutorials/s2-from-aws/index.html#review-query-result",
    "href": "tutorials/s2-from-aws/index.html#review-query-result",
    "title": "Access Sentinel 2 Data from AWS",
    "section": "Review Query Result",
    "text": "Review Query Result\nWe’ll use GeoPandas DataFrame object to make plotting easier.\n\ngdf = gpd.GeoDataFrame.from_features(stac_json, \"epsg:4326\")\n\n# Compute granule id from components\ngdf[\"granule\"] = (\n    gdf[\"mgrs:utm_zone\"].apply(lambda x: f\"{x:02d}\")\n    + gdf[\"mgrs:latitude_band\"]\n    + gdf[\"mgrs:grid_square\"]\n)\n\nfig = gdf.plot(\n    \"granule\",\n    edgecolor=\"black\",\n    categorical=True,\n    aspect=\"equal\",\n    alpha=0.5,\n    figsize=(6, 12),\n    legend=True,\n    legend_kwds={\"loc\": \"upper left\", \"frameon\": False, \"ncol\": 1},\n)\n_ = fig.set_title(\"STAC Query Results\")"
  },
  {
    "objectID": "tutorials/s2-from-aws/index.html#plot-stac-items-on-a-map",
    "href": "tutorials/s2-from-aws/index.html#plot-stac-items-on-a-map",
    "title": "Access Sentinel 2 Data from AWS",
    "section": "Plot STAC Items on a Map",
    "text": "Plot STAC Items on a Map\n\n# https://github.com/python-visualization/folium/issues/1501\nfig = Figure(width=\"400px\", height=\"500px\")\nmap1 = folium.Map()\nfig.add_child(map1)\n\nfolium.GeoJson(\n    shapely.geometry.box(*bbox),\n    style_function=lambda x: dict(fill=False, weight=1, opacity=0.7, color=\"olive\"),\n    name=\"Query\",\n).add_to(map1)\n\ngdf.explore(\n    \"granule\",\n    categorical=True,\n    tooltip=[\n        \"granule\",\n        \"datetime\",\n        \"s2:nodata_pixel_percentage\",\n        \"eo:cloud_cover\",\n    ],\n    popup=True,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"STAC\",\n    m=map1,\n)\n\nmap1.fit_bounds(bounds=convert_bounds(gdf.unary_union.bounds))\ndisplay(fig)"
  },
  {
    "objectID": "tutorials/s2-from-aws/index.html#construct-dask-dataset",
    "href": "tutorials/s2-from-aws/index.html#construct-dask-dataset",
    "title": "Access Sentinel 2 Data from AWS",
    "section": "Construct Dask Dataset",
    "text": "Construct Dask Dataset\nNote that even though there are 9 STAC Items on input, there is only one timeslice on output. This is because of groupby=\"solar_day\". With that setting stac_load will place all items that occured on the same day (as adjusted for the timezone) into one image plane.\n\n# Since we will plot it on a map we need to use `EPSG:3857` projection\ncrs = \"epsg:3857\"\nzoom = 2**5  # overview level 5\n\nxx = stac_load(\n    items,\n    bands=(\"red\", \"green\", \"blue\"),\n    crs=crs,\n    resolution=10 * zoom,\n    chunks={},  # &lt;-- use Dask\n    groupby=\"solar_day\",\n    stac_cfg=cfg,\n)\ndisplay(xx)"
  },
  {
    "objectID": "tutorials/s2-from-aws/index.html#load-data-and-convert-to-rgba",
    "href": "tutorials/s2-from-aws/index.html#load-data-and-convert-to-rgba",
    "title": "Access Sentinel 2 Data from AWS",
    "section": "Load data and convert to RGBA",
    "text": "Load data and convert to RGBA\n\n%%time\nrgba = to_rgba(xx, clamp=(1, 3000))\n_rgba = rgba.compute()"
  },
  {
    "objectID": "tutorials/s2-from-aws/index.html#display-image-on-a-map",
    "href": "tutorials/s2-from-aws/index.html#display-image-on-a-map",
    "title": "Access Sentinel 2 Data from AWS",
    "section": "Display Image on a map",
    "text": "Display Image on a map\n\nmap2 = folium.Map()\n\nfolium.GeoJson(\n    shapely.geometry.box(*bbox),\n    style_function=lambda x: dict(fill=False, weight=1, opacity=0.7, color=\"olive\"),\n    name=\"Query\",\n).add_to(map2)\n\ngdf.explore(\n    \"granule\",\n    categorical=True,\n    tooltip=[\n        \"granule\",\n        \"datetime\",\n        \"s2:nodata_pixel_percentage\",\n        \"eo:cloud_cover\",\n    ],\n    popup=True,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"STAC\",\n    m=map2,\n)\n\n\n# Image bounds are specified in Lat/Lon order with Lat axis inversed\nimage_bounds = convert_bounds(_rgba.geobox.geographic_extent.boundingbox, invert_y=True)\nimg_ovr = folium.raster_layers.ImageOverlay(\n    _rgba.isel(time=0).data, bounds=image_bounds, name=\"Image\"\n)\nimg_ovr.add_to(map2)\nmap2.fit_bounds(bounds=image_bounds)\n\nfolium.LayerControl().add_to(map2)\nfolium.plugins.Fullscreen().add_to(map2)\nmap2"
  },
  {
    "objectID": "tutorials/s2-from-aws/index.html#load-with-bounding-box",
    "href": "tutorials/s2-from-aws/index.html#load-with-bounding-box",
    "title": "Access Sentinel 2 Data from AWS",
    "section": "Load with bounding box",
    "text": "Load with bounding box\nAs you can see stac_load returned all the data covered by STAC items returned from the query. This happens by default as stac_load has no way of knowing what your query was. But it is possible to control what region is loaded. There are several mechanisms available, but probably simplest one is to use bbox= parameter (compatible with stac_client).\nLet’s load a small region at native resolution to demonstrate.\n\nr = 6.5 * km2deg\nsmall_bbox = (x - r, y - r, x + r, y + r)\n\nyy = stac_load(\n    items,\n    bands=(\"red\", \"green\", \"blue\"),\n    crs=crs,\n    resolution=10,\n    chunks={},  # &lt;-- use Dask\n    groupby=\"solar_day\",\n    stac_cfg=cfg,\n    bbox=small_bbox,\n)\nim_small = to_rgba(yy, clamp=(1, 3000)).compute()\n\n\nimg_zoomed_in = odc.ui.mk_data_uri(\n    odc.ui.to_jpeg_data(im_small.isel(time=0).data, quality=80), \"image/jpeg\"\n)\nprint(f\"Image url: {img_zoomed_in[:64]}...\")\n\n\nHTML(\n    data=f\"\"\"\n&lt;style&gt; .img-two-column{{\n  width: 50%;\n  float: left;\n}}&lt;/style&gt;\n&lt;img src=\"{img_zoomed_in}\" alt=\"Sentinel-2 Zoom in\" class=\"img-two-column\"&gt;\n&lt;img src=\"{img_ovr.url}\" alt=\"Sentinel-2 Mosaic\" class=\"img-two-column\"&gt;\n\"\"\"\n)"
  },
  {
    "objectID": "tutorials/landsat-from-mspc/README_devcontainer.html",
    "href": "tutorials/landsat-from-mspc/README_devcontainer.html",
    "title": "DevContainer Instructions",
    "section": "",
    "text": "DevContainer Instructions\nThis tutorial has been opened in a devcontainer. Use the Explorer view tree to open a notebook."
  }
]